{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleBaseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 : simplebaseline 모델 완성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simplebaseline.py 파일 내용을 완성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return seq_model\n",
    "\n",
    "upconv = _make_deconv_layer(3)\n",
    "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')\n",
    "\n",
    "def Simplebaseline(input_shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 : simplebaseline 모델로 변경하여 훈련하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.py 218라인의 모델 선언 부분을 simplebaseline 모델로 변경한 후 다시 학습을 진행합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# ...생략... train.py 참고\n",
    "\n",
    "def train(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, version, flag):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists('./models'):\n",
    "        os.makedirs('./models/')\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "        \n",
    "        if flag:\n",
    "            model = Simplebaseline(IMAGE_SHAPE) # IMAGE_SHAPE 인자 전달\n",
    "        else:\n",
    "            model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        \n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            start_epoch=start_epoch,\n",
    "            version=version,\n",
    "            tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n",
    "# 생략...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3 : 두 모델의 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습에서 다룬 StackedHourglass Network와 Simplebaseline 모델을 둘 다 동일한 Epoch 수만큼 학습하여 그 결과를 비교해 봅니다.\n",
    "\n",
    "- Pose Estimation 결과 시각화 (정성적 비교)\n",
    "- 학습 진행경과 (loss 감소현황)   \n",
    "\n",
    "가급적 두 모델 공히 최소 3epoch이상, (5epoch 이상 권장)을 학습하기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 모델의 Loss 감소 현황을 비교하기 위해 train.py의 run 메소드를 일부 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def run(self, train_dist_dataset, val_dist_dataset):\n",
    "    @tf.function\n",
    "    def distributed_train_epoch(dataset):\n",
    "        tf.print('Start distributed traininng...')\n",
    "        total_loss = 0.0\n",
    "        num_train_batches = 0.0\n",
    "        for one_batch in dataset:\n",
    "            per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                self.train_step, args=(one_batch, ))\n",
    "            batch_loss = self.strategy.reduce(\n",
    "                tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "            total_loss += batch_loss\n",
    "            num_train_batches += 1\n",
    "            tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                     batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "        return total_loss, num_train_batches\n",
    "\n",
    "    @tf.function\n",
    "    def distributed_val_epoch(dataset):\n",
    "        total_loss = 0.0\n",
    "        num_val_batches = 0.0\n",
    "        for one_batch in dataset:\n",
    "            per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                self.val_step, args=(one_batch, ))\n",
    "            num_val_batches += 1\n",
    "            batch_loss = self.strategy.reduce(\n",
    "                tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "            tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                     batch_loss)\n",
    "            if not tf.math.is_nan(batch_loss):\n",
    "                # TODO: Find out why the last validation batch loss become NaN\n",
    "                total_loss += batch_loss\n",
    "            else:\n",
    "                num_val_batches -= 1\n",
    "\n",
    "        return total_loss, num_val_batches\n",
    "\n",
    "    summary_writer = tf.summary.create_file_writer(self.tensorboard_dir)\n",
    "    summary_writer.set_as_default()\n",
    "\n",
    "    history = {'train_loss':[], 'val_loss':[]} # 두 모델의 정량적 평가를 위해 추가\n",
    "    for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "        tf.summary.experimental.set_step(epoch)\n",
    "\n",
    "        self.lr_decay()\n",
    "        tf.summary.scalar('epoch learning rate',\n",
    "                          self.current_learning_rate)\n",
    "\n",
    "        print('Start epoch {} with learning rate {}'.format(\n",
    "            epoch, self.current_learning_rate))\n",
    "\n",
    "        train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "            train_dist_dataset)\n",
    "        train_loss = train_total_loss / num_train_batches\n",
    "        history['train_loss'].append(train_loss) # 히스토리\n",
    "        print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "        tf.summary.scalar('epoch train loss', train_loss)\n",
    "\n",
    "        val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "            val_dist_dataset)\n",
    "        val_loss = val_total_loss / num_val_batches\n",
    "        history['train_loss'].append(val_loss) # 히스토리\n",
    "        print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "        tf.summary.scalar('epoch val loss', val_loss)\n",
    "\n",
    "        # save model when reach a new lowest validation loss\n",
    "        if val_loss < self.lowest_val_loss:\n",
    "            self.save_model(epoch, val_loss)\n",
    "            self.lowest_val_loss = val_loss\n",
    "        self.last_val_loss = val_loss\n",
    "\n",
    "    return self.best_model, history\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hourglass vs simplebaseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 결과로는 두 모델의 우열을 가리기 어렵다.   \n",
    "다만 골반쪽만 보면 hourglass가 조금 더 나아보인다.\n",
    "\n",
    "학습 경과로는 simplebaseline이 처음부터 낮은 Loss값으로 시작하고 변화폭이 상대적으로 작고, hourglass가 상대적으로 높은 Loss값으로 시작해 변화폭이 조금 더 크다.   \n",
    "다만 변화폭은 크게 차이나는 편은 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이썬 파일들을 적절히 수정하여 노드를 진행했다.   \n",
    "- 새롭게 작성해야 하는 부분은 없어서 진행 자체는 수월했지만, 학습에 걸리는 시간이 심각하게 오래 걸린다. \n",
    "- 둘 다 스크린샷을 찍어서 조금 차이가 있지만 비슷한 크기인데 왜 주피터 노트북 마크다운 문법을 이용하면 저렇게 다르게 나올까"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
